{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Movie Classification, the sequel\n",
    "![](https://images-na.ssl-images-amazon.com/images/S/sgp-catalog-images/region_US/paramount-01376-Full-Image_GalleryBackground-en-US-1484000188762._RI_SX940_.jpg)\n",
    "\n",
    "\n",
    "#### In this assignment, we will learn a little more about word2vec and then use the resulting vectors to make some predictions.\n",
    "\n",
    "We will be working with a movie synopsis dataset, found here: http://www.cs.cmu.edu/~ark/personas/\n",
    "\n",
    "The overall goal should sound a little familiar - based on the movie synopses, we will classify movie genre. Some of your favorites should be in this dataset, and hopefully, based on the genre specific terminology of the movie synopses, we will be able to figure out which movies are which type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: clean your dataset!\n",
    "\n",
    "For your input data:\n",
    "\n",
    "1. Find the top 10 movie genres\n",
    "2. Remove any synopses that don't fit into these genres\n",
    "3. Take the top 10,000 reviews in terms of \"Movie box office revenue\"\n",
    "\n",
    "Congrats, you've got a dataset! For each movie, some of them may have multiple classifications. To deal with this, you'll have to look at the Reuters dataset classification code that we used previously and possibly this example: https://github.com/keras-team/keras/blob/master/examples/reuters_mlp.py\n",
    "\n",
    "We want to use categorical cross-entropy as our loss function (or a one vs. all classifier in the case of SVM) because our data will potentially have multiple classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Deepak\\Desktop\\II SEMESTER\\NLP\\MovieSummaries\")\n",
    "\n",
    "\n",
    "movie = pd.read_csv(\"movie.metadata.tsv\",sep=\"\\t\",header=None)\n",
    "\n",
    "\n",
    "\n",
    "movie_header = [\"wikipedia_movie_id\", \"freebase_movie_id\", \"movie_name\",\n",
    "               \"movie_release_date\", \"movie_box_office_revenue\",\n",
    "               \"movie_runtime\", \"movie_languages\", \"movie_countries\",\n",
    "               \"movie_genres\"]\n",
    "movie.columns = movie_header\n",
    "movie.head()\n",
    "\n",
    "# Remove the NaN on the basis of Movie box office revenue\n",
    "movie = movie[movie['movie_box_office_revenue'].notnull()]\n",
    "\n",
    "def getVal(series):\n",
    "        aa_dict = ast.literal_eval(series)\n",
    "        val_list = []\n",
    "        for val in aa_dict.values():\n",
    "            val_list.append(val)\n",
    "        return val_list\n",
    "        \n",
    "              \n",
    "movie[[\"movie_languages\"]] = movie[[\"movie_languages\"]].applymap(lambda m:getVal(m))\n",
    "movie[[ \"movie_countries\"]] = movie[[ \"movie_countries\"]].applymap(lambda m:getVal(m))\n",
    "movie[[\"movie_genres\"]] = movie[['movie_genres']].applymap(lambda m:getVal(m))\n",
    "\n",
    "\n",
    "\n",
    "all_genre = list(movie['movie_genres'])\n",
    "\n",
    "all_genre_flat = [item for sublist in all_genre for item in sublist]\n",
    "\n",
    "movie_genre_count = Counter(all_genre_flat)        \n",
    "\n",
    "top_10_movie_genres = [item[0] for item in movie_genre_count.most_common(10)]\n",
    "\n",
    "keep_genre = []\n",
    "\n",
    "for item in all_genre:\n",
    "    genre = list(set(item).intersection(set(top_10_movie_genres)))\n",
    "    if len(genre)>0:\n",
    "        keep_genre.append(genre[0])\n",
    "    else:\n",
    "        keep_genre.append(np.nan)\n",
    "        \n",
    "movie['movie_genres'] = keep_genre\n",
    "\n",
    "movie = movie[movie['movie_genres'].notnull()]\n",
    "\n",
    "with open(\"plot_summaries.txt\", 'r',encoding='utf-8') as FR:\n",
    "       synopses = FR.readlines()\n",
    "\n",
    "synopses = {x.split('\\t')[0]:x.split('\\t')[1] for x in synopses}\n",
    "\n",
    "movie['synopses'] = [synopses[str(key)] if str(key) in synopses else np.nan for key in movie[\"wikipedia_movie_id\"]]\n",
    "\n",
    "movie = movie[movie['synopses'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shlykov, a hard-working taxi driver and Lyosha, a saxophonist, develop a bizarre love-hate relationship, and despite their prejudices, realize they aren't so different after all.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item,values in synopses.items():\n",
    "    print(values)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = movie.iloc[0:10,:]\n",
    "\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "def getsynopses(series):\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(series.lower())\n",
    "    processed_word_list = [i for i in tokens if i not in stop and len(i)>2]\n",
    "    return processed_word_list\n",
    "     \n",
    "#movie[['synopses']].applymap(lambda m:getsynopses(m))\n",
    "\n",
    "movie[['synopses']] = movie[['synopses']].applymap(lambda m:getsynopses(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = list(movie['synopses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "model = gensim.models.Word2Vec(X, iter=10, min_count=10, size=200, workers=4)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecArray = np.array([np.mean([w2v[w] for w in words if w in w2v] or [np.zeros(len(w2v))], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecArray_series = pd.Series(vecArray.tolist())\n",
    "\n",
    "genre = list(movie['movie_genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"X\":test,\"y\":genre})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Split the data\n",
    "\n",
    "Make a dataset of 70% train and 30% test. Sweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.X,df.y,test_size=0.3,stratify=df.y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSeries(series):\n",
    "        val_list = []\n",
    "        val_list.append(series)\n",
    "        return val_list\n",
    "        \n",
    "              \n",
    "y_train = y_train.apply(lambda m:getSeries(m))\n",
    "y_test = y_test.apply(lambda m:getSeries(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_movies_list = [['Drama'],\n",
    " ['Comedy'],\n",
    " ['Romance Film'],\n",
    " ['Thriller'],\n",
    " ['Action'],\n",
    " ['Action/Adventure'],\n",
    " ['Crime Fiction'],\n",
    " ['Adventure'],\n",
    " ['Indie'],\n",
    " ['Romantic comedy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: Build a model using ONLY word2vec\n",
    "\n",
    "Woah what? I don't think that's recommended...\n",
    "\n",
    "In fact it's a commonly accepted practice. What you will want to do is average the word vectors that will be input for a given synopsis (https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) and then input that averaged vector as your feature space into a model. For this example, use a Support Vector Machine classifier. For your first time doing this, train a model in Gensim and use the output vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "binarizer = MultiLabelBinarizer()\n",
    "binarizer.fit(top_movies_list)\n",
    "\n",
    "\n",
    "train_label = binarizer.fit_transform(y_train)\n",
    "test_label = binarizer.fit_transform(y_test)\n",
    "\n",
    "y_train = train_label\n",
    "y_test = test_label\n",
    "\n",
    "train_list  = []\n",
    "for val in X_train:\n",
    "    test.append(np.array(val))\n",
    "\n",
    "final_train_list = np.array(train_list)\n",
    "\n",
    "test_list  = []\n",
    "for val in X_test:\n",
    "    test3.append(np.array(val))\n",
    "\n",
    "final_test_list = np.array(test_list)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "model = classifier.fit(final_train_list, y_train)\n",
    "\n",
    "\n",
    "predictions = model.predict(final_test_list)\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def evaluate(test_labels, predictions):\n",
    "    precision = precision_score(test_labels, predictions, average='macro')\n",
    "    recall = recall_score(test_labels, predictions, average='macro')\n",
    "    accuracy = accuracy_score(test_labels,predictions)\n",
    "\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, Accuracy: {:.4f}\".format(precision, recall,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3870, Recall: 0.0862, Accuracy: 0.1163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: Do the same thing but with pretrained embeddings\n",
    "\n",
    "Now pull down the Google News word embeddings and do the same thing. Compare the results. Why was one better than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "model2 = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "movie = pd.read_csv(\"movie.metadata.tsv\",sep=\"\\t\",header=None)\n",
    "\n",
    "\n",
    "\n",
    "movie_header = [\"wikipedia_movie_id\", \"freebase_movie_id\", \"movie_name\",\n",
    "               \"movie_release_date\", \"movie_box_office_revenue\",\n",
    "               \"movie_runtime\", \"movie_languages\", \"movie_countries\",\n",
    "               \"movie_genres\"]\n",
    "movie.columns = movie_header\n",
    "movie.head()\n",
    "\n",
    "# Remove the NaN on the basis of Movie box office revenue\n",
    "movie = movie[movie['movie_box_office_revenue'].notnull()]\n",
    "\n",
    "def getVal(series):\n",
    "        aa_dict = ast.literal_eval(series)\n",
    "        val_list = []\n",
    "        for val in aa_dict.values():\n",
    "            val_list.append(val)\n",
    "        return val_list\n",
    "        \n",
    "              \n",
    "movie[[\"movie_languages\"]] = movie[[\"movie_languages\"]].applymap(lambda m:getVal(m))\n",
    "movie[[ \"movie_countries\"]] = movie[[ \"movie_countries\"]].applymap(lambda m:getVal(m))\n",
    "movie[[\"movie_genres\"]] = movie[['movie_genres']].applymap(lambda m:getVal(m))\n",
    "\n",
    "\n",
    "\n",
    "all_genre = list(movie['movie_genres'])\n",
    "\n",
    "all_genre_flat = [item for sublist in all_genre for item in sublist]\n",
    "\n",
    "movie_genre_count = Counter(all_genre_flat)        \n",
    "\n",
    "top_10_movie_genres = [item[0] for item in movie_genre_count.most_common(10)]\n",
    "\n",
    "keep_genre = []\n",
    "\n",
    "for item in all_genre:\n",
    "    genre = list(set(item).intersection(set(top_10_movie_genres)))\n",
    "    if len(genre)>0:\n",
    "        keep_genre.append(genre[0])\n",
    "    else:\n",
    "        keep_genre.append(np.nan)\n",
    "        \n",
    "movie['movie_genres'] = keep_genre\n",
    "\n",
    "movie = movie[movie['movie_genres'].notnull()]\n",
    "\n",
    "with open(\"plot_summaries.txt\", 'r',encoding='utf-8') as FR:\n",
    "       synopses = FR.readlines()\n",
    "\n",
    "synopses = {x.split('\\t')[0]:x.split('\\t')[1] for x in synopses}\n",
    "\n",
    "movie['synopses'] = [synopses[str(key)] if str(key) in synopses else np.nan for key in movie[\"wikipedia_movie_id\"]]\n",
    "\n",
    "movie = movie[movie['synopses'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "movie_mean_wordvec=np.zeros((len(movie),300))\n",
    "movie_mean_wordvec.shape\n",
    "\n",
    "movie_genres_list = list(movie['movie_genres'])\n",
    "synopses_list = list(movie['synopses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the movie mean wordvec\n",
    "\n",
    "genres=[]\n",
    "rows_to_delete=[]\n",
    "for i in range(len(movie)):\n",
    "    movie_genres=movie_genres_list[i]\n",
    "    genres.append(movie_genres)\n",
    "    overview=synopses_list[i]\n",
    "    tokens = tokenizer.tokenize(overview)\n",
    "    stopped_tokens = [k for k in tokens if not k in en_stop]\n",
    "    count_in_vocab=0\n",
    "    s=0\n",
    "    if len(stopped_tokens)==0:\n",
    "        rows_to_delete.append(i)\n",
    "        genres.pop(-1)\n",
    "    else:\n",
    "        for tok in stopped_tokens:\n",
    "            if tok.lower() in model2.vocab:\n",
    "                count_in_vocab+=1\n",
    "                s+=model2[tok.lower()]\n",
    "        if count_in_vocab!=0:\n",
    "            movie_mean_wordvec[i]=s/float(count_in_vocab)\n",
    "        else:\n",
    "            rows_to_delete.append(i)\n",
    "            genres.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for model\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mask2=[]\n",
    "for row in range(len(movie_mean_wordvec)):\n",
    "    if row in rows_to_delete:\n",
    "        mask2.append(False)\n",
    "    else:\n",
    "        mask2.append(True)\n",
    "\n",
    "X=movie_mean_wordvec[mask2]\n",
    "\n",
    "\n",
    "mlb=MultiLabelBinarizer()\n",
    "Y=mlb.fit_transform(genres)\n",
    "\n",
    "mask_text=np.random.rand(len(X))<0.8\n",
    "\n",
    "X_train=X[mask_text]\n",
    "Y_train=Y[mask_text]\n",
    "X_test=X[~mask_text]\n",
    "Y_test=Y[~mask_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Build a neural net model using word2vec embeddings (both pretrained and within an Embedding layer from Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model_textual = Sequential([\n",
    "    Dense(300, input_shape=(300,)),\n",
    "    Activation('relu'),\n",
    "    Dense(24),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model_textual.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5780/5780 [==============================] - 0s 50us/step - loss: 0.8358 - acc: 0.7271\n",
      "Epoch 2/10\n",
      "5780/5780 [==============================] - 0s 15us/step - loss: 0.8042 - acc: 0.7271\n",
      "Epoch 3/10\n",
      "5780/5780 [==============================] - 0s 15us/step - loss: 0.7966 - acc: 0.7271\n",
      "Epoch 4/10\n",
      "5780/5780 [==============================] - 0s 14us/step - loss: 0.7893 - acc: 0.7271\n",
      "Epoch 5/10\n",
      "5780/5780 [==============================] - 0s 14us/step - loss: 0.7827 - acc: 0.7271\n",
      "Epoch 6/10\n",
      "5780/5780 [==============================] - 0s 14us/step - loss: 0.7774 - acc: 0.7271\n",
      "Epoch 7/10\n",
      "5780/5780 [==============================] - 0s 14us/step - loss: 0.7730 - acc: 0.7271\n",
      "Epoch 8/10\n",
      "5780/5780 [==============================] - 0s 14us/step - loss: 0.7689 - acc: 0.7271\n",
      "Epoch 9/10\n",
      "5780/5780 [==============================] - 0s 14us/step - loss: 0.7666 - acc: 0.7271\n",
      "Epoch 10/10\n",
      "5780/5780 [==============================] - 0s 14us/step - loss: 0.7631 - acc: 0.7271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x272982196a0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_textual.fit(X_train, Y_train, epochs=10, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416/1416 [==============================] - 0s 118us/step\n"
     ]
    }
   ],
   "source": [
    "score1 = model_textual.evaluate(X_test, Y_test, batch_size=249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 73.34%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model_textual.metrics_names[1], score1[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Change the architecture of your model and compare the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(300,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(24))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27297eec710>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=10000, batch_size=500,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416/1416 [==============================] - 0s 23us/step\n"
     ]
    }
   ],
   "source": [
    "score = model_textual.evaluate(X_test, Y_test, batch_size=249)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: For each model, do an error evaluation\n",
    "\n",
    "You now have a bunch of classifiers. For each classifier, pick 2 good classifications and 2 bad classifications. Print the expected and predicted label, and also print the movie synopsis. From these results, can you spot some systematic errors from your models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 74.23%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model_textual.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_preds=model_textual.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
